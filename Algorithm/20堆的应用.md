> 100个文件，每个文件大小数量级百兆，每个文件中存储有序字符串，将100个文件合并成一个大文件。

从100个文件中各取第一个字符串放在数组中，然后遍历数组找到最小的元素，从数组中删除最小元素，放到大文件中，这样的复杂度是O(n)。如果最小字符串来自10.txt文件，再从这个文件中取出一个字符串放到数组中，以此类推。

如果用堆来做，从100个文件中各取出第一个字符串放堆中，形成小顶堆，每次最顶端的这个就是要删除的。如果堆顶的元素来自10.txt文件，再从这个文件取出一个元素放堆中，查找位置的过程时间复杂度O(logn)，然后再删除堆顶元素，重新排序，这个过程的时间复杂度也是O(logn)，所以用堆来做时间复杂度是O(logn)。

> 定时器的任务有不同时间执行，怎样做最高效？

一种做法是每秒扫描任务列表一次，这样做，首先会有徒劳的情况，因为当遍历到一个任务可能这个任务离真正执行还很远，另外每次遍历任务列表也很耗时。

用堆来做，每次把最先执行的任务放堆顶，轮询的时间间隔=当前时间-堆顶任务执行时间，每次执行堆顶任务后，把该任务从堆顶删除，再把最近要执行的任务放堆顶，这样又可以计算出轮询间隔时间了，这种方式最高效。


> 利用堆求topk

维护一个大小为k的的小顶堆，遍历数组与堆顶元素比较，如果比堆顶元素大，就把堆顶元素删除，再插入到小顶堆中。遍历数组的时间复杂度是O(n)，插入元素的时间复杂度是O(logk)，所以整个时间复杂度是O(nlogk)。

> 求由小到大数中的中位数

把前一半的数据放大顶堆里，把后一般的数据放小顶堆里。如果有动态插入数据，当插入的数据小于等于大顶堆堆顶元素，把这个数据插入到大顶堆；如果插入的数据大于等于小顶堆堆顶元素，就把这个数据插入到小顶堆；插入后会导致大顶堆和小顶堆的元素数量不相同，这时把数量多的那个堆的堆顶元素移动到数量少的那个堆顶上去。插入的时间复杂度O(logn)，取出堆顶元素的时间复杂度O(1)，所以整个的时间复杂度是O(logn)。

如果不是求中位数，比如求按9：1后分的数，道理也一样。

> 10亿搜索关键词，获取top10,单机，内存1GB

散列表存储10亿个关键词，建立10个结点的小顶堆，遍历散列表，最终获取top10。但这样做有问题，10亿条，假设有1亿条不重复，每个关键词50个字节，那1亿调5后面9个零，就是5G，可是内存只有1G。

把10亿个关键词切片放在10个文件中，遍历10亿个关键词，哈希算法求哈希值，哈希值和10取模，得到结果就是搜索关键词应该被分配的文件编号。假设每个文件有1000万个搜索关键词，每个关键词50个字节，5后面8个零，500后面6个零，500MB，每个切片文件现在可以被加载到内存了。好了，针对每个切片文件，建立小顶堆，最后有10个小顶堆，100个节点再排序。

> 访问量大的网站，每1小时显示top10新闻概要

根据每篇新闻摘要求出hash code,新闻摘要为散列表的值，每到新的小时，就把前面一个小时的新闻概要合并成一个文件。大网站，1个小时的数据量也会很大，所以准备把每个小时的大数据切分成不同的文件，对新闻概要求哈希值，然后再和某个固定数取模，将得数相同的放在相同的文件。然后对某个切片文件，来一个小顶堆，遍历切片文件，最终得到10个节点，再把每个切片的10个节点和其它欺骗的10个节点合并，求出最终每个小时的top10。根据每个小时的top10也很容易得到每天的top10。




